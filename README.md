# Autoformer ile Zaman Serisi Tahmini (ETTm1 Veri Seti)

Bu proje, uzun vadeli zaman serisi tahmini iÃ§in geliÅŸtirilmiÅŸ Autoformer (Auto-Correlation Transformer) modelini ETTm1 (Electricity Transforming Temperature) veri seti Ã¼zerinde uygulayarak Ã§ok deÄŸiÅŸkenli zaman serisi tahmini gerÃ§ekleÅŸtirmektedir. Proje, modern transformer mimarilerinin zaman serisi analizi alanÄ±ndaki potansiyelini gÃ¶stermeyi ve comprehensive bir analiz sunmayÄ± amaÃ§lar.

## ğŸ“‹ Ä°Ã§indekiler

- [Proje HakkÄ±nda](#proje-hakkÄ±nda)
- [AmaÃ§ ve Kapsam](#amaÃ§-ve-kapsam)
- [Veri Seti](#veri-seti)
- [Metodoloji](#metodoloji)
- [Model Mimarisi](#model-mimarisi)
- [Kurulum ve KullanÄ±m](#kurulum-ve-kullanÄ±m)
- [SonuÃ§lar ve Performans](#sonuÃ§lar-ve-performans)
- [GÃ¶rselleÅŸtirmeler](#gÃ¶rselleÅŸtirmeler)
- [KatkÄ±da Bulunma](#katkÄ±da-bulunma)

## ğŸš€ Proje HakkÄ±nda

Bu Ã§alÄ±ÅŸma, Autoformer modelinin temel bileÅŸenlerini (ayrÄ±ÅŸtÄ±rma bloklarÄ±, otomatik korelasyon mekanizmasÄ±) kullanarak elektrik transformatÃ¶r sÄ±caklÄ±k ve yÃ¼k verilerini analiz eder. **96 zaman adÄ±mÄ±** geÃ§miÅŸ veriyi kullanarak **96 zaman adÄ±mÄ±** gelecek tahmini gerÃ§ekleÅŸtiren model, Ã§ok deÄŸiÅŸkenli (multivariate) yaklaÅŸÄ±mla 7 farklÄ± Ã¶zelliÄŸi eÅŸ zamanlÄ± olarak tahmin eder.

### ğŸ¯ AmaÃ§ ve Kapsam

- **Ana Hedef**: 'OT' (Oil Temperature - YaÄŸ SÄ±caklÄ±ÄŸÄ±) ve diÄŸer Ã¶zellikleri Ã§ok deÄŸiÅŸkenli yaklaÅŸÄ±mla tahmin
- **Teknik AmaÃ§**: Transformer mimarilerinin zaman serisi verilerindeki potansiyelini gÃ¶sterme
- **Akademik KatkÄ±**: Autoformer'Ä±n auto-correlation mekanizmasÄ±nÄ± praktik uygulamada test etme
- **Metodolojik DeÄŸer**: KapsamlÄ± veri Ã¶n iÅŸleme, modelleme, eÄŸitim ve deÄŸerlendirme pipeline'Ä± sunma

## ğŸ“Š Veri Seti: ETTm1 (Electricity Transformer Temperature)

### Veri Seti Ã–zellikleri
- **Kaynak**: [ETDataset Repository](https://github.com/zhouhaoyi/ETDataset)
- **Zaman AralÄ±ÄŸÄ±**: 2016-07-01 ile 2018-06-26 (69,680 zaman adÄ±mÄ±)
- **Ã‡Ã¶zÃ¼nÃ¼rlÃ¼k**: 15 dakikalÄ±k Ã¶lÃ§Ã¼mler
- **Format**: CSV dosyasÄ±, eksik deÄŸer yok

### DeÄŸiÅŸkenler
| DeÄŸiÅŸken | AÃ§Ä±klama | TÃ¼r |
|----------|----------|-----|
| `date` | Zaman damgasÄ± | Datetime |
| `HUFL` | High Useful Load (YÃ¼ksek FaydalÄ± YÃ¼k) | Covariate |
| `HULL` | High Useless Load (YÃ¼ksek FaydasÄ±z YÃ¼k) | Covariate |
| `MUFL` | Medium Useful Load (Orta FaydalÄ± YÃ¼k) | Covariate |
| `MULL` | Medium Useless Load (Orta FaydasÄ±z YÃ¼k) | Covariate |
| `LUFL` | Low Useful Load (DÃ¼ÅŸÃ¼k FaydalÄ± YÃ¼k) | Covariate |
| `LULL` | Low Useless Load (DÃ¼ÅŸÃ¼k FaydasÄ±z YÃ¼k) | Covariate |
| `OT` | Oil Temperature (YaÄŸ SÄ±caklÄ±ÄŸÄ±) | **Ana Hedef** |

### Veri BÃ¶lÃ¼mÃ¼ Stratejisi
```
ğŸ“Š Toplam: 69,680 Ã¶rnek
â”œâ”€â”€ ğŸ¯ EÄŸitim (Train): 60% â†’ 41,808 Ã¶rnek
â”œâ”€â”€ âœ… DoÄŸrulama (Validation): 20% â†’ 13,936 Ã¶rnek  
â””â”€â”€ ğŸ§ª Test: 20% â†’ 13,936 Ã¶rnek
```

## ğŸ›  Metodoloji

### 3.1 Veri Ã–n Ä°ÅŸleme Pipeline'Ä±
```python
# 1. Zaman Ä°ndeksi DÃ¶nÃ¼ÅŸÃ¼mÃ¼
df['date'] = pd.to_datetime(df['date'])
df.set_index('date', inplace=True)

# 2. Standardizasyon
scaler = StandardScaler()
scaled_data = scaler.fit_transform(train_data)

# 3. Zaman Ã–zellik Ã‡Ä±karÄ±mÄ± (5 Ã¶zellik)
time_features = time_features(df.index, freq='t')

# 4. Sequence OluÅŸturma
seq_len = 96      # Girdi dizisi uzunluÄŸu (1 gÃ¼n)
label_len = 48    # Decoder baÅŸlangÄ±Ã§ token
pred_len = 96     # Tahmin ufku (1 gÃ¼n)
```

### 3.2 Autoformer Girdi FormatÄ±
Model iÃ§in 5 farklÄ± dizi tÃ¼rÃ¼ oluÅŸturulur:
- **X_enc**: (samples, 96, 7) - Encoder ana girdisi
- **X_mark_enc**: (samples, 96, 5) - Encoder zaman Ã¶zellikleri  
- **X_dec**: (samples, 144, 7) - Decoder girdisi (48 geÃ§miÅŸ + 96 sÄ±fÄ±r)
- **X_mark_dec**: (samples, 144, 5) - Decoder zaman Ã¶zellikleri
- **Y_true**: (samples, 96, 7) - Hedef deÄŸerler

## ğŸ— Model Mimarisi

### Autoformer KonfigÃ¼rasyonu
```python
Model Hiperparametreleri:
â”œâ”€â”€ Sequence Length: 96 (1 gÃ¼n geÃ§miÅŸ)
â”œâ”€â”€ Prediction Length: 96 (1 gÃ¼n tahmin)  
â”œâ”€â”€ Model Dimension: 512
â”œâ”€â”€ Attention Heads: 8
â”œâ”€â”€ Encoder Layers: 2
â”œâ”€â”€ Decoder Layers: 1
â”œâ”€â”€ Feed Forward: 2048
â”œâ”€â”€ Auto-correlation Factor: 3
â”œâ”€â”€ Moving Average Window: 25
â”œâ”€â”€ Dropout: 0.05
â””â”€â”€ Toplam Parametreler: ~10.5M
```

### Auto-Correlation MekanizmasÄ±
- **Geleneksel Self-Attention Alternatifi**: O(L log L) karmaÅŸÄ±klÄ±k
- **Periyodik Ã–rÃ¼ntÃ¼ Yakalama**: Zaman serisi iÃ§in optimize edilmiÅŸ
- **Trend-Seasonality AyrÄ±ÅŸtÄ±rmasÄ±**: Moving average ile decomposition

## ğŸ›  Kurulum ve KullanÄ±m

### Sistem Gereksinimleri
```
ğŸ Python: 3.11.12
ğŸ”¥ PyTorch: 2.6.0+cu124  
ğŸ¤— Transformers: 4.48.3
ğŸ“Š NumPy: 2.0.2
ğŸ“ˆ Pandas: 2.2.2
âš¡ CUDA: Desteklenen GPU (Ã¶nerilen)
```

### HÄ±zlÄ± BaÅŸlangÄ±Ã§
```bash
# 1. Repository'yi klonlayÄ±n
git clone https://github.com/kullaniciadi/autoformer-zaman-serisi-tahmini.git

# 2. Google Colab'da aÃ§Ä±n ve sÄ±rasÄ±yla Ã§alÄ±ÅŸtÄ±rÄ±n:
```

### AdÄ±m AdÄ±m Ã‡alÄ±ÅŸtÄ±rma
1. **HÃ¼cre 1**: KÃ¼tÃ¼phane kurulumu â†’ Runtime yeniden baÅŸlat
2. **HÃ¼cre 2**: KÃ¼tÃ¼phane import ve sÃ¼rÃ¼m kontrolÃ¼
3. **HÃ¼cre 3**: ETTm1 veri seti yÃ¼kleme ve keÅŸif
4. **HÃ¼cre 4**: Veri Ã¶n iÅŸleme ve sequence oluÅŸturma
5. **HÃ¼cre 5**: Autoformer kaynak kod entegrasyonu
6. **HÃ¼cre 6**: Model tanÄ±mlama ve konfigÃ¼rasyon
7. **HÃ¼cre 6a**: GeliÅŸmiÅŸ veri hazÄ±rlÄ±ÄŸÄ±
8. **HÃ¼cre 7**: Model eÄŸitimi (10 epoch)
9. **HÃ¼cre 8-11**: Test, deÄŸerlendirme ve gÃ¶rselleÅŸtirme

## ğŸ“ˆ SonuÃ§lar ve Performans

### EÄŸitim PerformansÄ±
| Epoch | Train Loss | Val Loss | Val MAE | Val RMSE | Val RÂ² |
|-------|------------|----------|---------|----------|--------|
| 1 | 0.3248 | 0.4466 | 0.4617 | 0.6678 | 0.4569 |
| 5 | 0.1421 | 0.3832 | 0.4235 | 0.6190 | 0.5106 |
| **10** | **0.0854** | **0.3684** | **0.4177** | **0.6064** | **0.5240** |

### Test Seti PerformansÄ± (Geri Ã–lÃ§eklendirilmiÅŸ)
```
ğŸ¯ Test Seti Metrikleri ('OT' Ã–zelliÄŸi):
â”œâ”€â”€ MSE: 5.830495
â”œâ”€â”€ MAE: 1.917781  
â”œâ”€â”€ RMSE: 2.414642
â”œâ”€â”€ RÂ²: 0.505747 (VaryansÄ±n %50.5'ini aÃ§Ä±klÄ±yor)
â””â”€â”€ MAPE: GÃ¼venilir deÄŸil (sÄ±fÄ±ra yakÄ±n deÄŸerler nedeniyle)
```

### Performans Analizi
- âœ… **RÂ² â‰ˆ 0.51**: Model test verisindeki varyansÄ±n %51'ini aÃ§Ä±klÄ±yor
- âœ… **Trend Yakalama**: Genel eÄŸilimleri baÅŸarÄ±yla modelliyor
- âœ… **Erken Durdurma**: 10. epoch'ta en iyi performans
- âš ï¸ **MAPE Problemi**: SÄ±fÄ±ra yakÄ±n gerÃ§ek deÄŸerler nedeniyle gÃ¼venilir deÄŸil

### SÃ¼re PerformansÄ±
```
â±ï¸ Performans Metrikleri:
â”œâ”€â”€ Toplam EÄŸitim SÃ¼resi: 19.47 dakika
â”œâ”€â”€ Ã‡Ä±karÄ±m HÄ±zÄ±: 0.001176 saniye/pencere
â””â”€â”€ Ä°ÅŸlem Kapasitesi: 850.68 pencere/saniye
```

## ğŸ“Š GÃ¶rselleÅŸtirmeler

### 1. EÄŸitim Ä°zleme Grafikleri
- **KayÄ±p Grafikleri**: Epoch bazÄ±nda train/validation loss tracking
- **Metrik EvolÃ¼syonu**: MAE, RMSE, RÂ² geliÅŸim grafikleri
- **Convergence Analizi**: Model yakÄ±nsama durumu

### 2. Tahmin Kalitesi Analizi  
- **GerÃ§ek vs Tahmin**: Zaman serisi overlay grafikleri
- **Hata DaÄŸÄ±lÄ±mÄ±**: Residual histogram ve density plots
- **Scatter Plot**: Predicted vs actual deÄŸerler

### 3. Ä°statistiksel Validasyon
- **ACF/PACF**: Residual'larÄ±n otokorelasyon analizi  
- **Residual Plot**: Sistematik hata kontrolÃ¼
- **Error Distribution**: Hata daÄŸÄ±lÄ±mÄ±nÄ±n normallik testi

## ğŸ”„ Proje Workflow

```mermaid
graph LR
    A[ETTm1 Veri YÃ¼kleme] --> B[Veri Ã–n Ä°ÅŸleme]
    B --> C[Zaman Ã–zellik Ã‡Ä±karÄ±mÄ±]  
    C --> D[Sequence OluÅŸturma]
    D --> E[Autoformer Modeli]
    E --> F[EÄŸitim & Validasyon]
    F --> G[Test & DeÄŸerlendirme]
    G --> H[GÃ¶rselleÅŸtirme & Analiz]
```

## ğŸ“ Akademik KatkÄ±lar

### Teknik Ä°novasyonlar
- **Auto-Correlation Mechanism**: Zaman serisi iÃ§in optimize edilmiÅŸ attention
- **Decomposition Architecture**: Trend-seasonal pattern ayrÄ±ÅŸtÄ±rmasÄ±
- **Multivariate Forecasting**: 7 Ã¶zellik eÅŸ zamanlÄ± tahmini

### Metodolojik DeÄŸer
- **Comprehensive Pipeline**: End-to-end zaman serisi analizi
- **Robust Evaluation**: Ã‡oklu metrik deÄŸerlendirmesi  
- **Reproducible Research**: DetaylÄ± dokÃ¼mantasyon ve kod

## ğŸ“ Proje YapÄ±sÄ±

```
autoformer-zaman-serisi-tahmini/
â”‚
â”œâ”€â”€ ğŸ““ Autoformer_ile_Zaman_Serisi_Tahmini.ipynb
â”œâ”€â”€ ğŸ“– README.md
â”œâ”€â”€ ğŸ“Š data/
â”‚   â””â”€â”€ ETTm1.csv (otomatik indirilir)
â”œâ”€â”€ ğŸ”§ autoformer_official_code/ (otomatik klonlanÄ±r)
â”œâ”€â”€ ğŸ’¾ models/
â”‚   â””â”€â”€ autoformer_best_model_v2.pth
â””â”€â”€ ğŸ“ˆ results/
    â”œâ”€â”€ training_plots/
    â”œâ”€â”€ prediction_plots/
    â””â”€â”€ performance_metrics/
```

## ğŸ¤ KatkÄ±da Bulunma

### GeliÅŸtirme AlanlarÄ±
- [ ] FarklÄ± sequence length'leri ile deneyim
- [ ] DiÄŸer ETT veri setleri (ETTh1, ETTh2, ETTm2) ile test
- [ ] Hyperparameter optimization
- [ ] Model ensemble teknikleri
- [ ] Real-time prediction pipeline

### KatkÄ± SÃ¼reci
1. Fork the repository
2. Create feature branch (`git checkout -b feature/yeni-ozellik`)
3. Commit changes (`git commit -am 'Yeni Ã¶zellik: XYZ'`)
4. Push to branch (`git push origin feature/yeni-ozellik`)
5. Create Pull Request

## ğŸ“š Referanslar

1. **Wu, H., Xu, J., Wang, J., & Long, M.** (2021). Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting. *Advances in Neural Information Processing Systems*, 34.

2. **Zhou, H., Zhang, S., Peng, J., Zhang, S., Li, J., Xiong, H., & Zhang, W.** (2021). Informer: Beyond efficient transformer for long sequence time-series forecasting. *AAAI 2021*.

3. **ETDataset Repository**: https://github.com/zhouhaoyi/ETDataset

## ğŸ™ TeÅŸekkÃ¼rler

- **THUML Research Group**: Orijinal Autoformer implementasyonu
- **ETDataset Contributors**: AÃ§Ä±k kaynak veri seti
- **PyTorch Team**: Deep learning framework
- **Google Colab**: Ãœcretsiz GPU kaynaklarÄ±a

## ğŸ“œ Lisans

Bu proje MIT lisansÄ± altÄ±nda lisanslanmÄ±ÅŸtÄ±r. Detaylar iÃ§in `LICENSE` dosyasÄ±na bakÄ±nÄ±z.

## ğŸ“ Ä°letiÅŸim

ğŸ› **Bug Report**: GitHub Issues kullanÄ±n  
ğŸ’¡ **Feature Request**: Discussions bÃ¶lÃ¼mÃ¼nden Ã¶nerinizi paylaÅŸÄ±n  
ğŸ“§ **Ä°letiÅŸim**: Repository sahibi ile iletiÅŸime geÃ§in
- E-posta: [mehmetaksoy49@gmail.com]


---

â­ **Bu projeyi beÄŸendiyseniz star vermeyi unutmayÄ±n!**

*Son gÃ¼ncelleme: AralÄ±k 2024*
